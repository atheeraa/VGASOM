{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLsN9aRmCD9f"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scipy networkx\n",
        "!pip install minisom\n",
        "!pip install --upgrade numpy\n",
        "import os\n",
        "from google.colab import drive\n",
        "MOUNTPOINT = '/content/gdrive'\n",
        "drive.mount(MOUNTPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLixHLLFroP9"
      },
      "outputs": [],
      "source": [
        "dataset_str = 'pubmed'\n",
        "epochs=100\n",
        "H1=32\n",
        "H2=16\n",
        "learning_rate = 0.001\n",
        "dropout = 0\n",
        "model_str = 'gcn_vae'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3zQD1_cr5SB"
      },
      "source": [
        "#VGAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUc7eJXIXjm"
      },
      "source": [
        "##Load the graph data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-_fUmyQIXGD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "\n",
        "datadir = os.path.join(MOUNTPOINT, '/content/gdrive/MyDrive/Colab Notebooks/gae-master/gae')\n",
        "# DATADIR = os.path.join(MOUNTPOINT, 'MyDrive/gae-master/gae/')\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "#still need to load data not from drive\n",
        "def load_data(dataset):\n",
        "    # load the data: x, tx, allx, graph\n",
        "    names = ['x', 'tx', 'allx', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"{}/data/ind.{}.{}\".format(datadir, dataset, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, tx, allx, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"{}/data/ind.{}.test.index\".format(datadir, dataset))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    # load the labels\n",
        "    lnames = ['y', 'ty', 'ally']\n",
        "    lobjects = []\n",
        "    for i in range(len(lnames)):\n",
        "      with open(\"{}/data/ind.{}.{}\".format(datadir, dataset, lnames[i]), 'rb') as f:\n",
        "        lobjects.append(pkl.load(f, encoding='latin1'))\n",
        "    y, ty, ally = tuple(lobjects)\n",
        "\n",
        "    if dataset == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :] #???\n",
        "    labels = np.argmax(labels,1)\n",
        "\n",
        "    return adj, features, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cStBz_PKAlm5",
        "outputId": "4ff5d5d6-b337-4bfd-942f-16eef0c5b1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w204qPQfIfxf"
      },
      "source": [
        "## Preprocess the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf3yr9JpIiBc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape\n",
        "\n",
        "\n",
        "def preprocess_graph(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    adj_ = adj + sp.eye(adj.shape[0])\n",
        "    rowsum = np.array(adj_.sum(1))\n",
        "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
        "    # construct feed dictionary\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
        "    feed_dict.update({placeholders['adj_orig']: adj})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def mask_test_edges(adj):\n",
        "    # Function to build test set with 10% positive links\n",
        "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "    # TODO: Clean up.\n",
        "\n",
        "    # Remove diagonal elements\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "    adj.eliminate_zeros()\n",
        "    # Check that diag is zero:\n",
        "    assert np.diag(adj.todense()).sum() == 0\n",
        "\n",
        "    adj_triu = sp.triu(adj)\n",
        "    adj_tuple = sparse_to_tuple(adj_triu)\n",
        "    edges = adj_tuple[0]\n",
        "    edges_all = sparse_to_tuple(adj)[0]\n",
        "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
        "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
        "\n",
        "    all_edge_idx = list(range(edges.shape[0]))\n",
        "    np.random.shuffle(all_edge_idx)\n",
        "    val_edge_idx = all_edge_idx[:num_val]\n",
        "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "    test_edges = edges[test_edge_idx]\n",
        "    val_edges = edges[val_edge_idx]\n",
        "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "    def ismember(a, b, tol=5):\n",
        "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    test_edges_false = []\n",
        "    while len(test_edges_false) < len(test_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], edges_all):\n",
        "            continue\n",
        "        if test_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                continue\n",
        "        test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    val_edges_false = []\n",
        "    while len(val_edges_false) < len(val_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], val_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], val_edges):\n",
        "            continue\n",
        "        if val_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                continue\n",
        "        val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    assert ~ismember(test_edges_false, edges_all)\n",
        "    assert ~ismember(val_edges_false, edges_all)\n",
        "    assert ~ismember(val_edges, train_edges)\n",
        "    assert ~ismember(test_edges, train_edges)\n",
        "    assert ~ismember(val_edges, test_edges)\n",
        "\n",
        "    data = np.ones(train_edges.shape[0])\n",
        "\n",
        "    # Re-build adj matrix\n",
        "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "    adj_train = adj_train + adj_train.T\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtrQ_dZfIoOb"
      },
      "source": [
        "## The optimizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuok4KtSItJJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "class OptimizerAE(object):\n",
        "    def __init__(self, preds, labels, pos_weight, norm):\n",
        "        preds_sub = preds\n",
        "        labels_sub = labels\n",
        "\n",
        "        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Adam Optimizer\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
        "                                           tf.cast(labels_sub, tf.int32))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
        "\n",
        "class OptimizerVAE(object):\n",
        "    def __init__(self, preds, labels, model, num_nodes, pos_weight, norm):\n",
        "        preds_sub = preds\n",
        "        labels_sub = labels\n",
        "        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Adam Optimizer\n",
        "        # Latent loss\n",
        "        self.log_lik = self.cost\n",
        "        self.kl = (0.5 / num_nodes) * tf.reduce_mean(tf.reduce_sum(1 + 2 * model.z_log_std - tf.square(model.z_mean) -\n",
        "                                                                   tf.square(tf.exp(model.z_log_std)), 1))\n",
        "        self.cost -= self.kl\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
        "                                           tf.cast(labels_sub, tf.int32))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fekHQH3MIuTr"
      },
      "source": [
        "## The vgae model: \n",
        "\n",
        "\n",
        "1.   Input dimensions\n",
        "2.   A GCN layer 32 hidden units  (Encoder) #you can change it in 2nd block\n",
        "3.   A GCN layer 16 hidden units  (Endcoder)#you can change it in 2nd block\n",
        "4.   A dot product Decoder = input dimensions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf9ycnwpIzsD"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "class GCNModelAE(Model):\n",
        "    def __init__(self, placeholders, num_features, features_nonzero, **kwargs):\n",
        "        super(GCNModelAE, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = num_features\n",
        "        self.features_nonzero = features_nonzero\n",
        "        self.adj = placeholders['adj']\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim,\n",
        "                                              output_dim=H1,\n",
        "                                              adj=self.adj,\n",
        "                                              features_nonzero=self.features_nonzero,\n",
        "                                              act=tf.nn.relu,\n",
        "                                              dropout=self.dropout,\n",
        "                                              logging=self.logging)(self.inputs)\n",
        "\n",
        "        self.embeddings = GraphConvolution(input_dim=H1,\n",
        "                                           output_dim=H2,\n",
        "                                           adj=self.adj,\n",
        "                                           act=lambda x: x,\n",
        "                                           dropout=self.dropout,\n",
        "                                           logging=self.logging)(self.hidden1)\n",
        "\n",
        "        self.z_mean = self.embeddings\n",
        "\n",
        "        self.reconstructions = InnerProductDecoder(input_dim=H2,\n",
        "                                      act=lambda x: x,\n",
        "                                      logging=self.logging)(self.embeddings)\n",
        "\n",
        "\n",
        "class GCNModelVAE(Model):\n",
        "    def __init__(self, placeholders, num_features, num_nodes, features_nonzero, **kwargs):\n",
        "        super(GCNModelVAE, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = num_features\n",
        "        self.features_nonzero = features_nonzero\n",
        "        self.n_samples = num_nodes\n",
        "        self.adj = placeholders['adj']\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim,\n",
        "                                              output_dim=H1,\n",
        "                                              adj=self.adj,\n",
        "                                              features_nonzero=self.features_nonzero,\n",
        "                                              act=tf.nn.relu,\n",
        "                                              dropout=self.dropout,\n",
        "                                              logging=self.logging)(self.inputs)\n",
        "\n",
        "        self.z_mean = GraphConvolution(input_dim=H1,\n",
        "                                       output_dim=H2,\n",
        "                                       adj=self.adj,\n",
        "                                       act=lambda x: x,\n",
        "                                       dropout=self.dropout,\n",
        "                                       logging=self.logging)(self.hidden1)\n",
        "\n",
        "        self.z_log_std = GraphConvolution(input_dim=H1,\n",
        "                                          output_dim=H2,\n",
        "                                          adj=self.adj,\n",
        "                                          act=lambda x: x,\n",
        "                                          dropout=self.dropout,\n",
        "                                          logging=self.logging)(self.hidden1)\n",
        "\n",
        "        self.z = self.z_mean + tf.random_normal([self.n_samples, H2]) * tf.exp(self.z_log_std)\n",
        "\n",
        "        self.reconstructions = InnerProductDecoder(input_dim=H2,\n",
        "                                      act=lambda x: x,\n",
        "                                      logging=self.logging)(self.z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA28fsD_JBtB"
      },
      "source": [
        "## Weight initialization \"Glorot uniform weight initialization\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr4kIdewJDvX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Glorot uniform weight initialization \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j79k0uxOI4J1"
      },
      "source": [
        "## Define the graph convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmvfRvokI6u8"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n",
        "        self.dropout = dropout\n",
        "        self.adj = adj\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "        x = tf.nn.dropout(x, 1-self.dropout)\n",
        "        x = tf.matmul(x, self.vars['weights'])\n",
        "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
        "        outputs = self.act(x)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparse(Layer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionSparse, self).__init__(**kwargs)\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n",
        "        self.dropout = dropout\n",
        "        self.adj = adj\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.features_nonzero = features_nonzero\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "        x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
        "        x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
        "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
        "        outputs = self.act(x)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class InnerProductDecoder(Layer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "        x = tf.transpose(inputs)\n",
        "        x = tf.matmul(inputs, x)\n",
        "        x = tf.reshape(x, [-1])\n",
        "        outputs = self.act(x)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX1EXn8DBX3c"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TshTm4LeJMPF"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sys import maxsize\n",
        "from numpy import set_printoptions\n",
        "\n",
        "set_printoptions(threshold=maxsize)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels = load_data(dataset_str)\n",
        "\n",
        "\n",
        "# Store original adjacency matrix (without diagonal entries) for later\n",
        "adj_orig = adj\n",
        "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
        "adj_orig.eliminate_zeros()\n",
        "\n",
        "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
        "adj = adj_train\n",
        "flag_features=1\n",
        "if flag_features == 0:\n",
        "    features = sp.identity(features.shape[0])  # featureless\n",
        "\n",
        "# Some preprocessing\n",
        "adj_norm = preprocess_graph(adj)\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'features': tf.sparse_placeholder(tf.float32),\n",
        "    'adj': tf.sparse_placeholder(tf.float32),\n",
        "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=())\n",
        "}\n",
        "\n",
        "num_nodes = adj.shape[0]\n",
        "\n",
        "features = sparse_to_tuple(features.tocoo())\n",
        "num_features = features[2][1]\n",
        "features_nonzero = features[1].shape[0]\n",
        "\n",
        "# Create model\n",
        "model = None\n",
        "\n",
        "if model_str == 'gcn_ae':\n",
        "    model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
        "elif model_str == 'gcn_vae':\n",
        "    model = GCNModelVAE(placeholders, num_features, num_nodes, features_nonzero)\n",
        "\n",
        "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
        "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
        "\n",
        "# Optimizer\n",
        "with tf.name_scope('optimizer'):\n",
        "    if model_str == 'gcn_ae':\n",
        "        opt = OptimizerAE(preds=model.reconstructions,\n",
        "                          labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
        "                                                                      validate_indices=False), [-1]),\n",
        "                          pos_weight=pos_weight,\n",
        "                          norm=norm)\n",
        "    elif model_str == 'gcn_vae':\n",
        "        opt = OptimizerVAE(preds=model.reconstructions,\n",
        "                           labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
        "                                                                       validate_indices=False), [-1]),\n",
        "                           model=model, num_nodes=num_nodes,\n",
        "                           pos_weight=pos_weight,\n",
        "                           norm=norm)\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "acc_val = []\n",
        "\n",
        "def get_roc_score(edges_pos, edges_neg, emb=None, save=False):\n",
        "    if emb is None:\n",
        "        feed_dict.update({placeholders['dropout']: 0})\n",
        "        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    adj_rec = np.dot(emb, emb.T)\n",
        "\n",
        "    #save the obtained embeddings\n",
        "    if save:\n",
        "      file_name = os.path.join(datadir, dataset_str + '_embeddings16dimensions.npy')\n",
        "      # with open('embeddings16dimensions.npy', 'wb') as f:\n",
        "      with open(file_name, 'wb') as f:\n",
        "        np.save(f, emb)\n",
        "\n",
        "      with open('adj_rec.npy', 'wb') as f:\n",
        "        np.save(f, adj_rec)\n",
        "\n",
        "    preds = []\n",
        "    pos = []\n",
        "    for e in edges_pos:\n",
        "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        pos.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_neg = []\n",
        "    neg = []\n",
        "    for e in edges_neg:\n",
        "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        neg.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    roc_score = roc_auc_score(labels_all, preds_all)\n",
        "    ap_score = average_precision_score(labels_all, preds_all)\n",
        "\n",
        "    return roc_score, ap_score\n",
        "\n",
        "\n",
        "cost_val = []\n",
        "acc_val = []\n",
        "val_roc_score = []\n",
        "e = []\n",
        "l = []\n",
        "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
        "adj_label = sparse_to_tuple(adj_label)\n",
        "# Train model\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: dropout})\n",
        "    # Run single weight update    \n",
        "    outs = sess.run([opt.opt_op, opt.cost, opt.accuracy, model.reconstructions], feed_dict=feed_dict)\n",
        "        \n",
        "    # Compute average loss\n",
        "    avg_cost = outs[1]\n",
        "    avg_accuracy = outs[2]\n",
        "    \n",
        "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
        "    val_roc_score.append(roc_curr)\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
        "          \"train_acc=\", \"{:.5f}\".format(avg_accuracy), \"val_roc=\", \"{:.5f}\".format(val_roc_score[-1]),\n",
        "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
        "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "    e.append(epoch+1)\n",
        "    l.append(avg_cost)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "roc_score, ap_score = get_roc_score(test_edges, test_edges_false, None, True)\n",
        "print('Test ROC score: ' + str(roc_score))\n",
        "print('Test AP score: ' + str(ap_score))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niMosvTk_USm"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r5xYmoh_T1f",
        "outputId": "3c302051-cf0e-40e6-9742-58955f535f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test ROC score: 0.9365197439922714\n",
            "Test AP score: 0.9351083339461435\n"
          ]
        }
      ],
      "source": [
        "print('Test ROC score: ' + str(roc_score))\n",
        "print('Test AP score: ' + str(ap_score))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvbW-t0FOIxz"
      },
      "source": [
        "# Save the obtained embeddings and true labels in separate files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bULW8eTVOLzp"
      },
      "outputs": [],
      "source": [
        "\n",
        "feed_dict.update({placeholders['dropout']: 0})\n",
        "emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
        "#save the obtained embeddings\n",
        "\n",
        "embeddings = os.path.join(datadir, dataset_str + '_embeddings.npy')\n",
        "with open(embeddings, 'wb') as f:\n",
        "  np.save(f, emb)\n",
        "\n",
        "truelabels = os.path.join(datadir, dataset_str + '_truelabels.npy')\n",
        "with open(truelabels, 'wb') as f:\n",
        "  np.save(f, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwAFYs-nHvPw"
      },
      "source": [
        "# Self-organizing map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATWqkDbGI9hy"
      },
      "source": [
        "### Install library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3gGbeg5L2fO"
      },
      "source": [
        "### Load the saved embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blO-jjx0LGsL"
      },
      "outputs": [],
      "source": [
        "from minisom import MiniSom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "\n",
        "file_name = os.path.join(datadir, dataset_str + '_embeddings.npy')\n",
        "with open(file_name, 'rb') as f:\n",
        "  data = np.load(f)\n",
        "data.shape\n",
        "\n",
        "file_name = os.path.join(datadir, dataset_str + '_truelabels.npy')\n",
        "with open(file_name, 'rb') as f:\n",
        "  labels = np.load(f)\n",
        "labels.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SK59ftQdq5F"
      },
      "outputs": [],
      "source": [
        "num = len(set(labels))\n",
        "clusters_num = len(set(labels))\n",
        "print(clusters_num)\n",
        "som_shape = (clusters_num, 1)\n",
        "dim = data.shape[1]\n",
        "som = MiniSom(som_shape[0],som_shape[1],dim, sigma=0.5, learning_rate=0.2,\n",
        "              neighborhood_function='mexican_hat', random_seed=10, topology ='rectangular',activation_distance='cosine')\n",
        "\n",
        "som.train(data, 50000, verbose=True)\n",
        "winner_coordinates = np.array([som.winner(x) for x in data]).T\n",
        "cluster_index = np.ravel_multi_index(winner_coordinates,som_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q5WuoB2H1NR"
      },
      "source": [
        "# Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFkwDG7wCts_"
      },
      "source": [
        "### clustering evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w73dCgRw7zBf"
      },
      "outputs": [],
      "source": [
        "!pip install munkres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I17m_sVjTXmO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn import metrics\n",
        "from munkres import Munkres, print_matrix\n",
        "\n",
        "class clustering_metrics():\n",
        "    def __init__(self, true_label, predict_label):\n",
        "        self.true_label = true_label\n",
        "        self.pred_label = predict_label\n",
        "\n",
        "\n",
        "    def clusteringAcc(self):\n",
        "        # best mapping between true_label and predict label\n",
        "        l1 = list(set(self.true_label))\n",
        "        numclass1 = len(l1)\n",
        "\n",
        "        l2 = list(set(self.pred_label))\n",
        "        numclass2 = len(l2)\n",
        "        if numclass1 != numclass2:\n",
        "            print('Class Not equal, Error')\n",
        "            return 0\n",
        "\n",
        "        cost = np.zeros((numclass1, numclass2), dtype=int)\n",
        "        for i, c1 in enumerate(l1):\n",
        "            mps = [i1 for i1, e1 in enumerate(self.true_label) if e1 == c1]\n",
        "            for j, c2 in enumerate(l2):\n",
        "                mps_d = [i1 for i1 in mps if self.pred_label[i1] == c2]\n",
        "\n",
        "                cost[i][j] = len(mps_d)\n",
        "\n",
        "        # match two clustering results by Munkres algorithm\n",
        "        m = Munkres()\n",
        "        cost = cost.__neg__().tolist()\n",
        "\n",
        "        indexes = m.compute(cost)\n",
        "\n",
        "        # get the match results\n",
        "        new_predict = np.zeros(len(self.pred_label))\n",
        "        for i, c in enumerate(l1):\n",
        "            # correponding label in l2:\n",
        "            c2 = l2[indexes[i][1]]\n",
        "\n",
        "            # ai is the index with label==c2 in the pred_label list\n",
        "            ai = [ind for ind, elm in enumerate(self.pred_label) if elm == c2]\n",
        "            new_predict[ai] = c\n",
        "\n",
        "        acc = metrics.accuracy_score(self.true_label, new_predict)\n",
        "        f1_macro = metrics.f1_score(self.true_label, new_predict, average='macro')\n",
        "        precision_macro = metrics.precision_score(self.true_label, new_predict, average='macro')\n",
        "        recall_macro = metrics.recall_score(self.true_label, new_predict, average='macro')\n",
        "        f1_micro = metrics.f1_score(self.true_label, new_predict, average='micro')\n",
        "        precision_micro = metrics.precision_score(self.true_label, new_predict, average='micro')\n",
        "        recall_micro = metrics.recall_score(self.true_label, new_predict, average='micro')\n",
        "        return acc, f1_macro, precision_macro, recall_macro, f1_micro, precision_micro, recall_micro\n",
        "\n",
        "    def evaluationClusterModelFromLabel(self):\n",
        "        # print('nmi')\n",
        "        nmi = metrics.normalized_mutual_info_score(self.true_label, self.pred_label)\n",
        "        # print('adjscore')\n",
        "        adjscore = metrics.adjusted_rand_score(self.true_label, self.pred_label)\n",
        "        acc, f1_macro, precision_macro, recall_macro, f1_micro, precision_micro, recall_micro = self.clusteringAcc()\n",
        "\n",
        "        print('ACC=%f, f1_macro=%f, nmi=%f, precision_macro=%f, recall_macro=%f, f1_micro=%f, precision_micro=%f, recall_micro=%f, NMI=%f, ADJ_RAND_SCORE=%f' % (acc, f1_macro, nmi, precision_macro, recall_macro, f1_micro, precision_micro, recall_micro, nmi, adjscore))\n",
        "\n",
        "        fh = open('recoder.txt', 'a')\n",
        "\n",
        "        fh.write('ACC=%f, f1_macro=%f, nmi=%f, precision_macro=%f, recall_macro=%f, f1_micro=%f, precision_micro=%f, recall_micro=%f, NMI=%f, ADJ_RAND_SCORE=%f' % (acc, f1_macro, nmi, precision_macro, recall_macro, f1_micro, precision_micro, recall_micro, nmi, adjscore) )\n",
        "        fh.write('\\r\\n')\n",
        "        fh.flush()\n",
        "        fh.close()\n",
        "\n",
        "        return acc, nmi, adjscore, f1_macro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQRcn6EgcH0l"
      },
      "outputs": [],
      "source": [
        "#Som evaluation\n",
        "cm = clustering_metrics(labels, cluster_index)\n",
        "c = cm.evaluationClusterModelFromLabel()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b3zQD1_cr5SB",
        "w204qPQfIfxf",
        "NtrQ_dZfIoOb",
        "JvbW-t0FOIxz",
        "JwAFYs-nHvPw",
        "MFkwDG7wCts_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}